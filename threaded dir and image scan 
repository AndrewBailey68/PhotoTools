import os
#import tkinter as tk
from concurrent.futures import ThreadPoolExecutor
import queue
import threading
import time
import logging
logging.basicConfig(level=logging.DEBUG)


Starttime = time.time()
maxthreadlist = [6]
max_threads = 1  # Set the maximum number of threads
thread_pool = ThreadPoolExecutor(max_threads)

# Globals
directory_path_Q = queue.Queue()  # global to contain temp list

image_directory_list = []  # list to hold directories with images
image_data_cache = []
data_cache_lock = threading.Lock()

image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.cr2', '.raw']
cv2_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']
raw_extensions = ['.cr2', '.raw']
# ===============================================
# Preprocess the image extensions to lowercase and convert them into a set for faster membership checking
image_extensions_set = {ext.lower() for ext in image_extensions}
cv2_extensions_set = {ext.lower() for ext in cv2_extensions}
raw_extension_set = {ext.lower() for ext in raw_extensions}

# test values
# Source directory where photos are stored
Source_list = ['/Users/andrew/Desktop', '/Users/andrew/Downloads', '/Users/andrew/Printrun']
#Source_list = ['/Users/andrew/Desktop/Test Folder For Images','/Volumes/media1/Pictures/Photos','/Volumes/media1/Pictures/Images removed from Photo library vault']
max_producer_threads = 2

def print_image_data_cache(image_data_cache_local):
    if not image_data_cache_local:
        print("Image data cache is empty.")
    else:
        print("Image Data Cache:")
        # for idx, data in enumerate(image_data_cache, start=1):
        #     print("\n".join([f"Filename: {data['filename']}, Path: {data['path']}"]))
    total_size = sum(data.get('size', 0) for data in image_data_cache_local)
    total_size_megabytes = total_size / (1024 * 1024)  # Convert bytes to megabytes
    formatted_size = '{:,.2f}'.format(total_size_megabytes)  # Format with commas and 2 decimal places
    print(f'Total size of image files: {formatted_size} MB')

# Create a dictionary to store unique image data based on filename and size
def deduplicate_image_cache(image_data_to_dedupe):
    unique_image_data = {}
    for data in image_data_to_dedupe:
        filename = data.get('filename')
        size = data.get('image_size')
        key = (filename, size)

        # Check if the key is already in the dictionary
        if key not in unique_image_data:
            unique_image_data[key] = data

    # Convert the dictionary values back to a list
    deduplicated_image_data = list(unique_image_data.values())
    return deduplicated_image_data


# enumerate directories into global Q
def count_directories(directoryqueue, root_directory):
    #print("Function: Count Directories")
    count = 0
    for current_root, dirs, _ in os.walk(root_directory):
        count += len(dirs)
        for dir_name in dirs:
            dir_path = os.path.join(current_root, dir_name)
            directoryqueue.put(dir_path)
            # print(f'Added Directory: {dir_path}')
    #print (f'Dir count {count}')
    return count


def find_images(temp_dir, data_cache_lock):
    logging.debug(f"Searching: {temp_dir}")

    if os.path.exists(temp_dir):
        image_flag = False
        for filename in os.listdir(temp_dir):
            file_path = os.path.join(temp_dir, filename)
            if not(os.path.isdir(file_path)):
            #if os.path.isfile(file_path) and (file_path.lower() in image_extensions_set):
                if file_path.lower().endswith(tuple(image_extensions)):
                    with data_cache_lock:
                        #logging.debug(f"Appending data to cache: {filename}")
                        image_data_cache.append(
                            {'filename': filename, 'path': temp_dir, 'size':os.path.getsize(file_path), 'type': file_path.lower})
                        #logging.debug(f"Appended data to cache: {filename}")
                    image_flag = True  # found at least one image
        if image_flag:
            image_directory_list.append(temp_dir)
    #print(f'Image Search Ended: {image_flag} in {temp_dir} ')
    return image_flag

def start_image_search(directoryqueue, data_cache_lock):
    # with ThreadPoolExecutor(max_workers = max_threads) as executor:
    #     while not(directory_path_Q.empty()):
    #         try:  # Add try...except block to catch any unexpected errors
    #             temp_dir = directory_path_Q.get()
    #             logging.debug(f"Consuming directory: {directory_path}")
    #             future = executor.submit(find_images, temp_dir, data_cache_lock)
    #             #print (future.result())
    #         except Exception as e:
    #             print(f"Error in start_image_search while loop: {str(e)}")
    sentinel_encountered = False
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        while not sentinel_encountered:
            try:
                temp_dir = directory_path_Q.get_nowait()
                logging.debug(f'popped {temp_dir}')
                if temp_dir is None:
                    sentinel_encountered = True
                    continue
                executor.submit(find_images, temp_dir, data_cache_lock)
            except queue.Empty:
                time.sleep(0.05)  # If the queue is empty, wait a bit and retry.
            
    print_image_data_cache(image_data_cache)
    logging.debug (f'found {len(image_data_cache)} images')
    
    
def start():
    master_list = []

    #Source_dir_Q = queue.Queue

    # while not Source_dir_Q.empty():
    #     # Check if less than 2 threads are active and if more directories need to be processed
    #     while threading.active_count() < max_producer_threads  and not Source_dir_Q.empty():
    #         # Create a new producer thread
    #         new_thread = threading.Thread(target=count_directories, args=(source_dir_q.get(),))
    #         producer_threads.append(new_thread)
    #         new_thread.start()
    #     producer_threads = [t for t in producer_threads if t.is_alive()]
    producer_threads= []
    for source_dir in Source_list[:max_producer_threads]: #start some producers
        if  os.path.exists(source_dir):
            producer_thread = threading.Thread(target=count_directories, args=(directory_path_Q, source_dir))
            logging.debug(f'producer thread set:{source_dir}')
            producer_thread.start()
            producer_threads.append(producer_thread)
            logging.debug(f'producer thread count: {len(producer_threads)}')

    # Starting Consumer Threads
    consumer_threads = []
    num_consumer_threads = max_threads  # or any number you prefer
    for _ in range(num_consumer_threads):
        consumer_thread = threading.Thread(target=start_image_search, args=(directory_path_Q, data_cache_lock))
        consumer_thread.start()
        consumer_threads.append(consumer_thread)

    # Wait for all producer threads to finish
    for thread in producer_threads:
        thread.join()        

    # Handling Graceful Shutdown of Consumer Threads
    for _ in consumer_threads:
        directory_path_Q.put(None)
    
    for thread in consumer_threads:
        thread.join()
        
        # for n in maxthreadlist:
        #     Starttime = time.time()
        #     max_threads = n
        #     start_image_search()
        #     print_image_data_cache(image_data_cache)
            
            
            # for image_data in image_data_cache:
            #     master_list.extend(image_data_cache)
            
    master_list = deduplicate_image_cache(master_list)
    print_image_data_cache(master_list)
    ElapsedTime = time.time() - Starttime
    logging.debug(f'{max_threads} Threads : {ElapsedTime} s')


start()