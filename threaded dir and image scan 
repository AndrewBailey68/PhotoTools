import os
#import tkinter as tk
import concurrent.futures
import queue
import threading
import time
import logging
import json
logging.basicConfig(level=logging.INFO)

output_file = '/Users/andrew/python/PhotoTools/datafile.json'

maxthreadlist = [6]

waittime = 0.05
Starttime = time.time()

max_producer_threads = 4
max_threads = 6  # Set the maximum number of threads
thread_pool = concurrent.futures.ThreadPoolExecutor(max_threads)
max_failed_attempts = 100

# Globals
directory_path_Q = queue.Queue()  # global to contain temp list
producer_dir_Q = queue.Queue()  #queue to hold the top level source dirs

image_directory_list = []  # list to hold directories with images
image_data_cache = []
data_cache_lock = threading.Lock()

image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.cr2', '.raw']
cv2_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']
raw_extensions = ['.cr2', '.raw']
# ===============================================
# Preprocess the image extensions to lowercase and convert them into a set for faster membership checking
image_extensions_set = {ext.lower() for ext in image_extensions}
cv2_extensions_set = {ext.lower() for ext in cv2_extensions}
raw_extension_set = {ext.lower() for ext in raw_extensions}

# test values
# Source directory where photos are stored
#pic a test source or a 'real' network one by comments
#Source_list=['/Users/andrew/Desktop/Test Folder For Images']
#Source_list = ['/Users/andrew/Desktop']
#Source_list = ['/Users/andrew/Desktop', '/Users/andrew/Downloads', '/Users/andrew/Printrun', '/Users/andrew/Documents']
Source_list = ['/Users/andrew/Desktop']
Source_list.append('/Volumes/media1/Pictures/Ioana Photos and Video')
Source_list.append ('/Volumes/media1/Pictures/Photos')
Source_list.append ('/Volumes/media1/Pictures/V2 - Aperture Library - old.photoslibrary_prepare')
Source_list.append ('/Volumes/media1/Pictures/100GOPRO')
Source_list.append ('/Volumes/media1/Pictures/Images removed from Photo library vault')
Source_list.append ('/Volumes/media1/Pictures/Images removed from Ultra6 Vault')
Source_list.append ('/Volumes/media1/Pictures/Camera Uploads')
Source_list.append ('/Volumes/media1/Pictures/Camera')
Source_list.append ('/Volumes/media1/Pictures/Scan 5')
Source_list.append ('None')

def can_access(path):
    #read permission available?
    return os.access(path, os.R_OK)

def print_image_data_cache(image_data_cache_local):
    if not image_data_cache_local:
        logging.debug ("finishing up - Image data cache is empty")
    else:
        logging.debug("Image Data Cache:")
        for idx, data in enumerate(image_data_cache, start=1):
            logging.debug("\n".join([f"Filename: {data['filename']}, Path: {data['path']}"]))
   
    total_size = sum(data.get('size', 0) for data in image_data_cache_local)
    total_size_megabytes = total_size / (1024 * 1024)  # Convert bytes to megabytes
    formatted_size = '{:,.2f}'.format(total_size_megabytes)  # Format with commas and 2 decimal places
    logging.info(f'Total size of image files: {formatted_size} MB')

# Create a dictionary to store unique image data based on filename and size
def deduplicate_image_cache(image_data_to_dedupe):
    unique_image_data = {}
    logging.info (f'Dedupe {len(image_data_to_dedupe)} images')
    for data in image_data_to_dedupe:
        filename = data.get('filename')
        size = data.get('image_size')
        key = (filename, size)

        # Check if the key is already in the dictionary
        if key not in unique_image_data:
            unique_image_data[key] = data

    # Convert the dictionary values back to a list
    deduplicated_image_data = list(unique_image_data.values())
    logging.info (f'Now {len(deduplicated_image_data)} images')
    return deduplicated_image_data

# enumerate directories into global Q
def count_directories(directoryqueue, root_directory):
    count = 0
    for current_root, dirs, _ in os.walk(root_directory):
        count += len(dirs)
        for dir_name in dirs:
            dir_path = os.path.join(current_root, dir_name)
            directoryqueue.put(dir_path)
            logging.debug(f'Added Directory: {dir_path}')
    logging.debug (f'Dir count {count} from {root_directory}')
    return count

def directory_search(source_dir_path):
    return count_directories (directory_path_Q, source_dir_path)

def process_directory(directory_path, data_cache_lock):
    # Logic to process a directory and find images
    # Utilizing the previously defined find_images function
    # Returns: found data
    return find_images(directory_path)

def find_images(temp_dir, threadlock_ref):
    logging.debug(f"Searching: {temp_dir}")
    if os.path.exists(temp_dir):    #check the path exists
        if can_access(temp_dir):    #check we have rights to access directory
            try:
                os.listdir(temp_dir)    #list what's in the directory
            except:
                logging.warning (f'Err:Find Images:Listing {temp_dir}')
                return (f'Fail {temp_dir}')
            image_flag= False   #set flag to false before the file scan
            try:
                for filename in os.listdir(temp_dir):
                    file_path = os.path.join(temp_dir, filename)
                    if not(os.path.isdir(file_path)) and can_access(temp_dir):
                    #if os.path.isfile(file_path) and (file_path.lower() in image_extensions_set)
                        if file_path.lower().endswith(tuple(image_extensions)):
                            with threadlock_ref:
                                #logging.debug(f"Appending data to cache: {filename}")
                                image_data_cache.append(
                                    {'filename': filename, 'path': temp_dir, 'size':os.path.getsize(file_path), 'type': os.path.splitext(file_path)[1][1:]})
                                logging.info(f"{threading.current_thread().name} Appended data to cache: {filename}")
                            image_flag = True  # found at least one image
                    else:   #it's a directory move on
                        pass        
                #logging.debug(f'Image Search Ended {image_flag} in {temp_dir}')
                return (f'Success {temp_dir}')
            except:
                logging.warning (f'Err:Find images {temp_dir}')
                return (f'fail {temp_dir}')
        else:
            logging.warning (f'Err:Find Images:No Access to {temp_dir}')
            return (f'no access: {temp_dir}')
    else:
        logging.warning (f'Err:Find Images:no path {temp_dir}')
        return (f'no path:{temp_dir}')


def start_image_search_concurrent(max_threads):
    all_image_data = []
# Is there a directory in the Q
# is it 'None' - then we are done
# if so set up a new thread
#if not, pause and go again
#wait until threads finish

    sentinel_encountered = False
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads, thread_name_prefix= 'find_image') as executor:
        while not (sentinel_encountered):
            try:
                temp_dir = directory_path_Q.get(0.1)
                if temp_dir == None:
                    sentinel_encountered = True
                else:
                    logging.debug (f'thread submission:{temp_dir}')
                    try:
                        future = executor.submit(find_images, temp_dir, data_cache_lock)
                    except:
                        logging.debug (f'Failed submission:{temp_dir}')
            except Exception as exc:
                logging.debug(f'Failure to get temp_dir{str(exc)}')
            for futures in concurrent.futures.as_completed():
                logging.debug (f'Completed:{future.result()}')
    
def start_image_search_threads(max_threads):
    sentinel_encountered = False
    counter = 0
    threads = []
    while not (sentinel_encountered):
        threads = [t for t in threads if t.is_alive()]  #thread cleanup
        if len(threads)< max_threads and directory_path_Q.qsize()>0:
            try:
                temp_dir = directory_path_Q.get(timeout=0.1)
            except Exception as exc:
                if exc == queue.Empty:
                    logging.debug ('image_search_threads:Failed to get path from Q')
                    time.sleep(waittime) #delay to allow the queue to be topped up
            if temp_dir == None:
                sentinel_encountered = True
            else:
                logging.debug (f'thread submission:{temp_dir}')
                try:
                    consumer_thread = threading.Thread(target=find_images, args=(temp_dir, data_cache_lock))
                except Exception as exc: 
                    logging.debug (f'thread submission failed: {temp_dir}: (str)exc))')
                    return
                threads.append(consumer_thread)
                consumer_thread.start() 
                directory_path_Q.task_done()
                #logging.debug (f'image search thread count: {len(threads)}')
        else:
            if directory_path_Q.qsize()==0 and counter<max_failed_attempts:
                counter += 1
                #logging.debug (f'Q empty {counter}')
            elif counter==max_failed_attempts:
                return
            else:
                #logging.debug (f'thread count>max threads')
                time.sleep(waittime)
                
def start_producer_queue_threads(max_producer_threads):
    sentinel_encountered = False
    counter = 0
    threads = []
    while not (sentinel_encountered):
        threads = [t for t in threads if t.is_alive()]  #thread cleanup
        if len(threads)< max_producer_threads and producer_dir_Q.qsize()>0: #alive threads <max? Work to do?
            try:
                temp_dir = producer_dir_Q.get(timeout=0.1)  #get the next item
                if temp_dir == 'none' or temp_dir == 'None' or temp_dir == None: #did we reach the end? Note potential error condition if producer_dir_Q.get() doesn't get anything.
                    logging.info ('Source dir list sent to threads')
                    sentinel_encountered = True
            except Exception as exc:
                if exc == queue.Empty:
                    logging.debug ('ProducerQ_threads:Failed to get path from Q')
                    time.sleep(waittime) #delay to allow the queue to be topped up
            else:
                logging.debug (f'ProducerQ thread submission:{temp_dir}')
                try:
                    producer_thread = threading.Thread(target=count_directories, args=(directory_path_Q, temp_dir))
                except Exception as exc: 
                    logging.debug (f'ProducerQ thread submission failed: {temp_dir}: (str)exc))')
                    return
                threads.append(producer_thread)
                producer_thread.start() 
                #producer_dir_Q.task_done()
                logging.debug (f'ProducerQ thread count: {len(threads)}')
        else:
            if directory_path_Q.qsize()==0 and counter<max_failed_attempts:
                counter += 1
                logging.debug (f'ProducerQ empty {counter}')
            elif counter==max_failed_attempts:
                logging.debug ('Producer Q max attempts')
                return
            else:
                #logging.debug (f'thread count>max threads')
                time.sleep(waittime)

def handle_non_serializable(obj):
    if callable(obj):
        return str(obj)
    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

def write_list_of_ngram_dicts(list_of_dicts, filename):
    with open(filename, 'w', encoding='utf-8') as file:
        for dic in list_of_dicts:
        
            data=json.dumps(dic) 
            file.write(data)
            file.write("\n")
    file.close
    while not(file.closed):
        time.sleep(waittime)

def start():
    master_list = []

    for max_producer_threads in maxthreadlist:
        for max_threads in maxthreadlist:
            Starttime = time.time()
            #Source_dir_Q = queue.Queue

            for source_dir in Source_list: #start some producers
                producer_dir_Q.put(source_dir)
            producer_dir_Q.put(None)
            start_producer_queue_threads(max_producer_threads)

            # Starting Consumer Threads
            num_consumer_threads = max_threads  # or any number you prefer
            time.sleep(waittime) #give the producer threads a chance to get ahead enough
            start_image_search_threads (num_consumer_threads)

            logging.info (f'found {len(image_data_cache)} images')                
            print_image_data_cache(image_data_cache)
            ElapsedTime = time.time() - Starttime
            logging.info(f'{max_producer_threads} producer {max_threads} image Threads : {ElapsedTime} s')
            
start()
master_list = deduplicate_image_cache(image_data_cache)
print_image_data_cache(master_list)
image_data_cache = master_list
write_list_of_ngram_dicts(image_data_cache, output_file)